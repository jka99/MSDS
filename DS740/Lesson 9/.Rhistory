data3 + geom_bar(position = "fill") +
xlab("Posting Model") +
ylab("Proportion Retained") +
ggtitle("One Day Retention After Posting Model Change") +
guides(fill = guide_legend(title = "Retention")) +
scale_fill_manual(labels = c("No", "Yes"), values = c("#FF8C00", "#66CD00")) +
scale_x_discrete(labels = c("Limited", "Unlimited")) +
guides(fill=guide_legend(title="Retention"))
test_data <- data %>%
drop_na()
# test_data %>% count(version, retention_1)
# limited FALSE    = 23442
# limited TRUE     = 19063
# limited sample   = 42505
# unlimited False  = 24051
# unlimited TRUE   = 19117
# unlimited sample = 43168
# samples <- c(42505, 43168)
# measures <- c(19063, 19117)
# prop.test(measures, samples, alternative = "two.sided", conf.level = 0.95, correct = F)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(tidyverse)
library(GGally)
library(caret)
library(rcompanion)
library(InformationValue)
data <- OJ
colSums(is.na(data))
# data %>% ggpairs(c("PriceCH", "PriceMM", "DiscCH", "DiscMM", "LoyalCH",
#                    "SalePriceMM", "SalePriceCH", "PriceDiff",
#                    "PctDiscMM", "PctDiscCH", "ListPriceDiff"))
###thinking we need a line of glms with graduAlly increasing number of variables add on. We can then pull all that together to show which variables were valuable and could lead us to great models while these other variables did make thge cut.
train.index <- createDataPartition(data$Purchase, p = 0.6,
list = FALSE, times = 1)
train.df <- data[train.index, ]
test.df <- data[-train.index, ]
# model.1 <- glm(Purchase ~ DiscCH + DiscMM
#                + LoyalCH + SalePriceMM + SalePriceCH,
#                data=train.df, family = "binomial")
# pred.p1 <- predict(model.1, newdata=train.df, type = "response")
#
# pred.class1 <- factor(ifelse(pred.p1 > 0.5, "Yes", "No"))
# cmat1 <- caret::confusionMatrix(pred.class1, train.df$Purchase,
#                                 positive = "1")
# ###Transformed data
# train.df$DiscCH.t <- transformTukey(train.df$DiscCH, plotit = F, quiet = T)
# train.df$DiscMM.t <- transformTukey(train.df$DiscMM, plotit = F, quiet = T)
train.index <- createDataPartition(data$Purchase, p = 0.6,
list = F, times = 1)
train.df <- data[train.index, ]
test.df <- data[-train.index, ]
train.df$DiscCH.t <- transformTukey(train.df$DiscCH, plotit = F, quiet = T)
train.df$DiscMM.t <- transformTukey(train.df$DiscMM, plotit = F, quiet = T)
model.t <- glm(Purchase ~ LoyalCH + SalePriceMM + SalePriceCH
+ DiscCH.t + DiscMM.t,
data=train.df, family = "binomial")
summary(model.t)
model <- glm(Purchase ~ LoyalCH + SalePriceMM + SalePriceCH,
data=train.df, family = "binomial")
pred <- predict(model, newdata=train.df, type = "response")
train.df$Purchase <- factor(ifelse(train.df$Purchase=="CH", "Yes", "No"))
pred.class <- factor(ifelse(pred > 0.5, "Yes", "No"))
opt.cut <- optimalCutoff(actuals = train.df$Purchase, predictedScores = pred,
optimiseFor = "misclasserror",
returnDiagnostics = TRUE)
#paste("Optimal Cutoff Threshold =", round(opt.cut$optimalCutoff, 3))
opt_cut <- round(opt.cut$optimalCutoff, 3)
test.df$Purchase <- factor(ifelse(test.df$Purchase=="CH", "Yes", "No"))
opt.pred.class <- (factor(ifelse(pred > opt_cut, "Yes", "No")))
#cmat3 <- caret::confusionMatrix(opt.pred.class, test.df$Purchase,
#                                positive = "Yes")
# model <- glm(Purchase ~ + LoyalCH + SalePriceMM + SalePriceCH,
#                data=train.df, family = "binomial")
# summary(model)
# pred <- predict(model, newdata=train.df, type = "response")
#
# train.df$Purchase <- (ifelse(train.df$Purchase=="CH", 0, 1))
#
# pred.class <- ifelse(pred > 0.5, 0, 1)
#
# cmat2 <- caret::confusionMatrix(pred.class, train.df$Purchase,
#                                 positive = "No")
# summary(model)
#
# table(pred)
# table(train.df$Purchase)
# table(opt.pred.class)
# cmat2
#
# opt.cut <- optimalCutoff(actuals = train.df$Purchase, predictedScores = pred,
#                          optimiseFor = "misclasserror",
#                          returnDiagnostics = TRUE)
# paste("Optimal Cutoff Threshold =", round(opt.cut$optimalCutoff, 3))
#
# opt.pred.class <- (factor(ifelse(pred > 0.007, "Yes", "No")))
# cmat3 <- caret::confusionMatrix(opt.pred.class, test.df$Purchase,
#                                 positive = "Yes")
# cmat3
#
# #test.pred <- predict(train.df, test.df)
# data <- data %>%
#   mutate(StoreID_asfactor = as.factor(StoreID))
#
# model <- glm(Purchase ~ WeekofPurchase + as.factor(StoreID) + DiscCH +
#                DiscMM + SpecialCH + SpecialMM + LoyalCH + SalePriceMM +
#                SalePriceCH + PriceDiff + PctDiscMM + PctDiscCH +
#                ListPriceDiff, data = data, family = binomial)
# model1 <- glm(Purchase == "CH" ~ ., data = data, family = binomial)
# summary(model)
# summary(model1)
# Use all of the variables that you deemed to be potentially useful for predicting customers who purchase Minute Maid orange juice on the training set.  If you transformed any of them, be sure to use the transformed version and omit the original variable.  Using the default cutoff threshold of 0.5, construct a confusion matrix and find the accuracy of predicting customers who purchased Minute Maid for the training set.
# Using the same variables, find the predictions for purchasing Minute Maid in the validation set.  Find a cutoff threshold that minimizes misclassification error, construct a confusion matrix and find the accuracy of predicting customers who purchased Minute Maid for the validation set.
# Compare the accuracy in the training set to the accuracy from the validation set. # Does it appear that overfitting is a problem here?
# What is the prevalence of customers who purchased Minute Maid in the validation set?  Note that this would be the accuracy with a “naïve” prediction of saying that all customers will be predicted to purchase the orange juice with the highest prevalence. How does this compare to the predicted accuracy using the model?
# What is the sensitivity of this predictive model on the validation set?  Explain what this means.
# What is the specificity of this predictive model on the validation set?  Explain what this means.
# Construct an ROC curve for the validation data and compute the Area Under the Curve (AUC). Interpret what these convey about this model’s predictive capability.
# With this data. What effect does balancing the data using ROSE have on the accuracy, sensitivity, specificity and AUC?
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(GGally)
library(gtsummary)
library(flextable)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(stringr)
library(stargazer)
library(tidyverse)
library(tidyr)
library(broom)
library(car)
library(ggpubr)
IM_20 = read.csv("IM_Florida_20.csv")
IM_21 = read.csv("IM_Florida_21.csv")
IM_20 <- IM_20 %>%
filter(FinishStatus == "Finisher")
IM_21 <- IM_21 %>%
filter(FinishStatus == "Finisher")
#Convert the swim, bike, run, and overall times from strings to numeric minutes.
IM_20 <- IM_20 %>%
mutate(OverallTime = hms(OverallTime),SwimTime = hms(SwimTime),
BikeTime = hms(BikeTime), RunTime = hms(RunTime))
IM_21 <- IM_21 %>%
mutate(OverallTime = hms(OverallTime),SwimTime = hms(SwimTime),
BikeTime = hms(BikeTime), RunTime = hms(RunTime))
IM_20 <- IM_20 %>%
mutate(OverallTime = (hour(OverallTime)*60) + minute(OverallTime) + (second(OverallTime)/60),
SwimTime = (ifelse ((hour(SwimTime) > 3),
(hour(SwimTime) + (minute(SwimTime)/60)),
(hour(SwimTime)*60) + minute(SwimTime) + (second(SwimTime)/60))),
BikeTime = (hour(BikeTime)*60) + minute(BikeTime) + (second(BikeTime)/60),
RunTime = (hour(RunTime)*60) + minute(RunTime) + (second(RunTime)/60),
TransTime = OverallTime - (SwimTime + BikeTime + RunTime))
IM_21 <- IM_21 %>%
mutate(OverallTime = (hour(OverallTime)*60) + minute(OverallTime) + (second(OverallTime)/60),
SwimTime = (ifelse ((hour(SwimTime) > 3),
(hour(SwimTime) + (minute(SwimTime)/60)),
(hour(SwimTime)*60) + minute(SwimTime) + (second(SwimTime)/60))),
BikeTime = (hour(BikeTime)*60) + minute(BikeTime) + (second(BikeTime)/60),
RunTime = (hour(RunTime)*60) + minute(RunTime) + (second(RunTime)/60),
TransTime = OverallTime - (SwimTime + BikeTime + RunTime))
#Create an AgeGroup variable that based on the Division (that notes age without gender). Note that individuals in the PRO and PC divisions do not have an age group reported.
#For this I advise looking into text manipulation packages and functions like grep or substring. The age group is all but the first character of division name.
IM_20 <- IM_20 %>%
mutate(
AgeGroup = str_sub(IM_20$Division, -5),
AgeRank = case_when(
AgeGroup == "18-24" ~ 1,
AgeGroup == "25-29" ~ 2,
AgeGroup == "30-34" ~ 3,
AgeGroup == "35-39" ~ 4,
AgeGroup == "40-44" ~ 5,
AgeGroup == "45-49" ~ 6,
AgeGroup == "50-54" ~ 7,
AgeGroup == "55-59" ~ 8,
AgeGroup == "60-64" ~ 9,
AgeGroup == "65-69" ~ 10,
AgeGroup == "70-74" ~ 11,
AgeGroup == "75-79" ~ 12
))
IM_21 <- IM_21 %>%
mutate(
AgeGroup = str_sub(IM_21$Division, -5),
AgeRank = case_when(
AgeGroup == "18-24" ~ 1,
AgeGroup == "25-29" ~ 2,
AgeGroup == "30-34" ~ 3,
AgeGroup == "35-39" ~ 4,
AgeGroup == "40-44" ~ 5,
AgeGroup == "45-49" ~ 6,
AgeGroup == "50-54" ~ 7,
AgeGroup == "55-59" ~ 8,
AgeGroup == "60-64" ~ 9,
AgeGroup == "65-69" ~ 10,
AgeGroup == "70-74" ~ 11,
AgeGroup == "75-79" ~ 12
))
# Join 20 and 21 data
IM_20_21 <- inner_join(IM_20, IM_21, by = "Name", suffix = c(".20", ".21"))
IM_20_21 <- IM_20_21 %>%
filter(AgeRank.20 <= AgeRank.21 & AgeRank.21 - AgeRank.20 < 2)
#Generate a scatterplot matrix
IMDF <- IM_20_21 %>%
select(SwimTime.20, BikeTime.20, RunTime.20, Gender.20, OverallTime.21)
ggpairs(IMDF, columns = c(1:3,5), aes(color = Gender.20))
#Fit four simple linear regression models predicting overall time in 2021; one model for each 2020 time metric (swim, bike, run, transition) as a predictor variable.
model_swim <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20)
model_bike <- lm(data = IM_20_21, OverallTime.21 ~ BikeTime.20)
model_run <- lm(data = IM_20_21, OverallTime.21 ~ RunTime.20)
model_trans <- lm(data = IM_20_21, OverallTime.21 ~ TransTime.20)
#https://www.roelpeters.be/how-to-add-a-regression-equation-and-r-squared-in-ggplot2/
# Swim
ggplot(aes(x = SwimTime.20, y = OverallTime.21), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(legend.position = "none") +
labs(x = "2020 Swim Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..))
ggplot(aes(x = SwimTime.20, y = OverallTime.21, color = Gender.20), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Swim Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Gender.20)
# Bike
ggplot(aes(x = BikeTime.20, y = OverallTime.21), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(legend.position = "none") +
labs(x = "2020 Bike Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..))
ggplot(aes(x = BikeTime.20, y = OverallTime.21, color = Gender.20), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Bike Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1050, aes(label = ..adj.rr.label..)) +
facet_wrap(~Gender.20)
#Run
ggplot(aes(x = RunTime.20, y = OverallTime.21), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(legend.position = "none") +
labs(x = "2020 Run Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..))
ggplot(aes(x = RunTime.20, y = OverallTime.21, color = Gender.20), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Run Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1050, aes(label = ..adj.rr.label..)) +
facet_wrap(~Gender.20)
#Create a forest plot that shows the estimated slope parameter and associated confidence interval for each 2020 event time component (swim, bike, run, transition).
df <- data.frame(characteristic = c("SwimTime.20", "BikeTime.20", "RunTime.20", "TransTime.20"),
coefficient = c(5.963, 2.059, 1.4449, 8.979),
Lower = c(4.837, 1.771, 1.238, 7.253),
Upper = c(7.09, 2.348, 1.66, 10.705),
p_value = c("<0.001", "<0.001", "<0.001", "<0.001"),
index = c(1,2,3,4))
ggplot(data = df, aes(y=characteristic, x=coefficient, xmin=Lower, xmax = Upper)) +
geom_point() +
geom_errorbarh(height=0.1)
#Fit a multiple regression model that small and interpretable, but still fits the data well. This will likely take some iteration. Do not show every step that you take in this process. Instead, narrate broadly the process that you followed and share details on the final model that you chose.
mlm_model.1 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20)
mlm_model.2 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 * BikeTime.20 * RunTime.20 * TransTime.20)
mlm_model.3 <- lm(data = IM_20_21, OverallTime.21 ~ (SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20)^2)
mlm_model.4 <- lm(data = IM_20_21, OverallTime.21 ~ (SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20)^4)
mlm_model.5 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20 + SwimTime.20:BikeTime.20 + SwimTime.20:RunTime.20)
mlm_model.6 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20 + SwimTime.20:BikeTime.20)
mlm_model.7 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + SwimTime.20:BikeTime.20)
mlm_null <- lm(data = IM_20_21, OverallTime.21 ~ 1)
# step(mlm_model.1, scope = list(lower = mlm_null, upper = mlm_model.1), direction = "both")
# summary(mlm_model.4)
# anova(mlm_model.6)
# summary(mlm_model.6)
#summary(mlm_model.2)
tidy(mlm_model.1)
tidy(mlm_model.2)
tidy(mlm_model.3)
tidy(mlm_model.4)
tidy(mlm_model.5)
tidy(mlm_model.6)
tidy(mlm_model.7)
glance(mlm_model.1)
glance(mlm_model.2)
glance(mlm_model.3)
glance(mlm_model.4)
glance(mlm_model.5)
glance(mlm_model.6)
glance(mlm_model.7)
IM_20_21.female <- IM_20_21 %>%
filter(Gender.20 == "Female")
IM_20_21.male <- IM_20_21 %>%
filter(Gender.20 == "Male")
mlm_model.7f <- lm(data = IM_20_21.female, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + SwimTime.20:BikeTime.20)
mlm_model.7m <- lm(data = IM_20_21.male, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + SwimTime.20:BikeTime.20)
summary(mlm_model.7)
summary(mlm_model.7f)
summary(mlm_model.7m)
#Use your chosen multiple regression model to address the primary research question. You may choose to report and interpret estimated model coefficients (possibly create another forest plot). You may choose to report and interpret R-squared or adjusted R-squared. Include any plots or tables that help support your explanation.
avPlots(mlm_model.7, main = "MLR Model for Ironman 2020-2021")
avPlots(mlm_model.7f, main = "MLR Model for Ironman 2020-2021: Females")
avPlots(mlm_model.7m, main = "MLR Model for Ironman 2020-2021: Males")
IM_21 = IM_21 %>% mutate(Group = case_when(
Name %in% IM_20_21$Name ~ "Repeat",
TRUE ~ "Non-repeat"))
IM_21_Repeat <- IM_21 %>%
filter(Group == "Repeat")
IM_21_Nonrepeat <- IM_21 %>%
filter(Group == "Non-repeat")
print(paste("The mean SwimTime for repeat athletes is", round(mean(IM_21_Repeat$SwimTime, na.rm = T),2)))
print(paste("The mean SwimTime for non-repeat athletes is", round(mean(IM_21_Nonrepeat$SwimTime, na.rm = T),2)))
#print(paste("The mean BikeTime for repeat athletes is",r_bike_mean))
r_bike_mean <- round(mean(IM_21_Repeat$BikeTime, na.rm = T),2)
#print(paste("The mean BikeTime for non-repeat athletes is", round(mean(IM_21_Nonrepeat$BikeTime, na.rm = T),2)))
r_swim_mean <- round(mean(IM_21_Repeat$SwimTime, na.rm = T),2)
nr_swim_mean <- round(mean(IM_21_Nonrepeat$SwimTime, na.rm = T),2)
r_bike_mean <- round(mean(IM_21_Repeat$BikeTime, na.rm = T),2)
nr_bike_mean <- round(mean(IM_21_Nonrepeat$BikeTime, na.rm = T),2)
r_run_mean <- round(mean(IM_21_Repeat$RunTime, na.rm = T),2)
nr_run_mean <- round(mean(IM_21_Nonrepeat$RunTime, na.rm = T),2)
r_overall_mean <- round(mean(IM_21_Repeat$OverallTime, na.rm = T),2)
nr_overall_mean <- round(mean(IM_21_Nonrepeat$OverallTime, na.rm = T),2)
print(paste("The mean BikeTime for repeat athletes is",r_swim_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_swim_mean),2)
print(paste("The mean BikeTime for repeat athletes is",r_bike_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_bike_mean),2)
print(paste("The mean BikeTime for repeat athletes is",r_run_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_run_mean),2)
print(paste("The mean BikeTime for repeat athletes is",r_overall_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_overall_mean),2)
ggplot(data = IM_21, aes(x = SwimTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Swim Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
ggplot(data = IM_21, aes(x = BikeTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Bike Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
ggplot(data = IM_21, aes(x = RunTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Run Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
gghistogram(IM_21, x = "OverallTime",
add = "mean", rug = F,
color = "Group", fill = "Group")
A <- 10
?mhv()
library(MVTests)
?mhz
library(MASS)  #help(qda)
library(pROC)
# for assumption-checking
library(mvnormalTest)  # of multivariate normality; or library(MVN) or (mvnTest)
library(MVTests)  # of constant covariance; or library(biotools) or (biotools) or (heplots)
# for data organization
library(dplyr)
# for visuals
library(ggformula)
?mhz
?pmax
pmin( c( 8, 1, 8 ), c(2, 7, 8) )
1- (4/3)
0.91*0.91
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggformula)
library(kernlab)
library(caret)
summary(iris)
my_iris <- iris %>%
filter(Species != "setosa") %>%
mutate(Species = factor(Species,
levels = c("versicolor",
"virginica")))
my_iris %>%
gf_point(Petal.Length ~ Sepal.Length,
color =~ Species,
pch =~ Species)
#library(kernlab)
set.seed(524)
data_used = my_iris
ctrl = trainControl(method = "cv", number = 10)
fit_iris = train(Species ~ Sepal.Length + Petal.Length,
data = data_used,
method = "svmLinear",
tuneGrid = expand.grid(C = c(.001, .01, .1, 1, 5, 10, 100)), # Costs to compare.  Decimals are OK.
preProcess = c("center","scale"),
trControl = ctrl)
fit_iris
fit_iris$finalModel
attr(fit_iris$finalModel, "SVindex")
my_iris <- my_iris %>%
tibble::rownames_to_column("Row") %>%
mutate(is_SV = Row %in% attr(fit_iris$finalModel, "SVindex"))
my_iris %>%
filter(is_SV) %>%
gf_point(Petal.Length ~ Sepal.Length,
pch =~Species, size = 2.5) %>%
gf_point(Petal.Length ~ Sepal.Length,
color =~ Species,
pch =~ Species, data = my_iris) %>%
gf_labs(title = "Highlighted points are support vectors")
my_iris <- my_iris %>%
mutate(scale_Petal.Length = scale(Petal.Length),
scale_Sepal.Length = scale(Sepal.Length))
b = attr(fit_iris$finalModel, "b")
b # -beta_0
coefs = attr(fit_iris$finalModel, "coef")[[1]]
head(coefs)
iris_SV <- my_iris %>%
filter(is_SV) %>%
select(c(scale_Sepal.Length, scale_Petal.Length)) %>%
as.matrix()
head(iris_SV) # Columns should be in the order x, y
# relative to the graph
w = colSums(coefs * iris_SV) # beta_1, ... beta_p
w
my_iris %>%
gf_point(scale_Petal.Length ~ scale_Sepal.Length,
color =~ Species,
pch =~ Species, data = my_iris) %>%
gf_abline(intercept = b/w[2], slope = -w[1]/w[2]) %>%
gf_abline(intercept = (b+1)/w[2], slope = -w[1]/w[2], lty = 2) %>%
gf_abline(intercept = (b-1)/w[2], slope = -w[1]/w[2], lty = 2)
# Convert the slope and y-intercept to the units of the unscaled data
x = my_iris$Sepal.Length
y = my_iris$Petal.Length
sd.ratio = sd(y)/sd(x)
yint = (sd(y) * b/w[2]) +
(w[1]/w[2] * mean(x) * sd.ratio) + mean(y)
my_slope = sd.ratio * -w[1]/w[2]
# Find the y-intercepts of the margin lines, in the units of the unscaled data
yint_upper = (sd(y) * (b+1)/w[2]) +
(w[1]/w[2] * mean(x) * sd.ratio) + mean(y)
yint_lower = (sd(y) * (b-1)/w[2]) +
(w[1]/w[2] * mean(x) * sd.ratio) + mean(y)
my_iris %>%
gf_point(Petal.Length ~ Sepal.Length,
color =~ Species,
pch =~ Species) %>%
gf_abline(intercept = yint, slope = my_slope) %>%
gf_abline(intercept = yint_lower, slope = my_slope,
lty = 2) %>%
gf_abline(intercept = yint_upper, slope = my_slope,
lty = 2)
head(predict(fit_iris))
conf_mat = table(predict(fit_iris), my_iris$Species)
conf_mat
# Accuracy
sum(diag(conf_mat)) / dim(my_iris)[1]  # (48+46) / 100
fit_iris$results[3, ]
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/lesson 9")
library(readr)
library(dplyr)
library(caret)
library(kernlab)
library(ggformula)
# Problem 4
bank = read_delim("bank-additional.csv", delim = ";")
bank <- bank %>%
mutate(scale_emp.var.rate = scale(emp.var.rate),
scale_duration = scale(duration))
bank %>%
gf_point(scale_duration ~ scale_emp.var.rate,
color =~ y, pch =~ y)
set.seed(999)
data_used = bank
ctrl = trainControl(method = "cv", number = 10)
fit_bank = train(y ~ emp.var.rate + duration,
data = data_used,
method = "svmLinear",
tuneGrid = expand.grid(C = 1),
preProcess = c("center","scale"),
trControl = ctrl)
fit_bank$results$Accuracy
conf_mat = table(predict(fit_bank), bank$y)
conf_mat
bank <- bank %>%
tibble::rownames_to_column("Row")
bank <- bank %>%
mutate(is_SV = Row %in% attr(fit_bank$finalModel, "SVindex"))
b = attr(fit_bank$finalModel, "b")
coefs = attr(fit_bank$finalModel, "coef")[[1]]
bank_SV <- bank %>%
filter(is_SV) %>%
select(c(scale_emp.var.rate, scale_duration)) %>%
as.matrix()
w = colSums(coefs * bank_SV) # beta_1, ... beta_p
bank %>%
gf_point(scale_duration ~ scale_emp.var.rate,
color =~ y, pch =~ y) %>%
gf_abline(intercept = b/w[2], slope = -w[1]/w[2]) %>%
gf_abline(intercept = (b+1)/w[2], slope = -w[1]/w[2], lty = 2) %>%
gf_abline(intercept = (b-1)/w[2], slope = -w[1]/w[2], lty = 2)
install.packages(c("caret", "cli", "data.table", "dplyr", "e1071", "fansi", "glmnet", "htmltools", "ipred", "lubridate", "tibble", "utf8", "vctrs", "xfun"))
