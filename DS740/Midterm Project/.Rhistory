X_train = model.matrix(Model.rfe, data = myAuto.reduced)[,-1]
y_train = myAuto.reduced$logRetailprice
result_rfe = rfe(x = X_train, y = y_train,
sizes = c(1:dim(myAuto.reduced)[2]-1),
rfeControl = control_rfe)
plot(result_rfe, type = c("g", "o"), main = "Recursive Feature Elimination: Reduced Model")
plot(result_rfe$results$RMSE[0:length(result_rfe$optVariables)+1] ~ c(0:length(result_rfe$optVariables)),
xaxt="n", type = "l", xlab = "", ylab = "RMSE Repeated Cross-Validation", main = "Recursive Feature Elimination: Reduced Model")
points(result_rfe$results$RMSE[0:length(result_rfe$optVariables)+1] ~ c(0:length(result_rfe$optVariables)), pch = 20, cex = 2)
axis(side = 1, at = 0:length(result_rfe$optVariables), labels = c('Intercept',result_rfe$optVariables), las = 2)
# REDUCED MODEL WITH ENET
# Some predictors are automatically removed
alphalist = seq(0, 1, by = 0.1)
lambdalist = c((1:1000)/10000)
training = trainControl(method = "cv", number = 5)
myAuto.reduced.ENET = train(logRetailprice ~ ., data = myAuto.reduced,
method = "glmnet", trControl = training,
tuneGrid = expand.grid(alpha=alphalist,
lambda=lambdalist))
coef(myAuto.reduced.ENET$finalModel, s = myAuto.reduced.ENET$bestTune$lambda)
myAuto.reduced.ENET.RMSE <- min(myAuto.reduced.ENET$results$RMSE)
paste("ENET RMSE: ",myAuto.reduced.ENET.RMSE)
# REDUCED MODEL WITH ROBUST
Model.Robust = (logRetailprice ~ logHorsepower + TypeRWD + TypeAWD + SUV + Sport + Wagon + Minivan + Pickup)
training = trainControl(method = "cv", number = 5)
myAuto.reduced.ROBUST = train(Model.Robust, data = myAuto.reduced,
method = "rlm", trControl = training)
coef(myAuto.reduced.ROBUST$finalModel)
myAuto.reduced.ROBUST.RMSE <- min(myAuto.reduced.ROBUST$results$RMSE)
paste("ROBUST RMSE: ",myAuto.reduced.ROBUST.RMSE)
##################################################
## DOUBLE CROSS VALIDATION
##################################################
library(doParallel)
myAuto <- myAuto.reduced
cls = makeCluster(detectCores()-1)
registerDoParallel(cls)
set.seed(9)
#tuning params for ENET
lambdalist = c((1:1000)/10000)
alphalist = seq(0, 1, by = 0.1)
n = dim(myAuto)[1]
n.alpha = length(alphalist)
nfolds = 5
groups = rep(1:nfolds,length=n)
cvgroups = sample(groups,n)
# store predicted values from the double-cross-validation
allpredictedCV = rep(NA,n)
allpredictedMethod = rep(NA,n)
allpredictedMSE = rep(NA,n)
# set up storage to see what models are "best" on the inner loops
allbestRMSE = rep(NA,nfolds)
allbestTypes = rep(NA,nfolds)
allbestPars = vector("list",nfolds)
allbestModel = vector("list",nfolds)
Model.Full = (logRetailprice ~ .)
Model.Penalized = (logRetailprice ~ .)
Model.Robust = (logRetailprice ~ logHorsepower + TypeRWD + TypeAWD + SUV + Sport + Wagon + Minivan + Pickup)
timing <- system.time({
for (j in 1:nfolds)  { # loop through outer splits
groupj = (cvgroups == j)
# train data
traindata = myAuto[!groupj,]
trainx = model.matrix(Model.Full, data = traindata)[,-1]
trainy = traindata$logRetailprice
# test data
validdata = myAuto[groupj,]
validx = model.matrix(Model.Full, data = validdata)[,-1]
validy = validdata$logRetailprice
# all model-fitting process with traindata
dataused=traindata
training = trainControl(method = "cv", number = 5, allowParallel = TRUE)
# cross-validation of penalized regression
fit_caret_penalized = train(Model.Penalized, data = dataused,
method = "glmnet", trControl = training,
tuneGrid = expand.grid(alpha=alphalist,lambda=lambdalist))
# cross-validation of robust regression
fit_caret_robust = train(Model.Robust, data = dataused, method = "rlm",
trControl = training)
# all best models
all_best_Types = c("ELASTICNET", "ROBUST")
all_best_Pars = list(fit_caret_penalized$bestTune, fit_caret_robust$bestTune)
all_best_Models = list(glmnet(trainx, trainy, alpha=fit_caret_penalized$bestTune$alpha, lambda=lambdalist),
fit_caret_robust$finalModel)
all_best_RMSE = c(min(fit_caret_penalized$results$RMSE),min(fit_caret_robust$results$RMSE))
# the best model of each fold
one_best_Type = all_best_Types[which.min(all_best_RMSE)]
one_best_Pars = all_best_Pars[which.min(all_best_RMSE)]
one_best_Model = all_best_Models[[which.min(all_best_RMSE)]]
one_best_RSME = all_best_RMSE[[which.min(all_best_RMSE)]]
# for checking later to see what's the best from each fold
allbestTypes[j] = one_best_Type
allbestPars[[j]] = one_best_Pars
allbestRMSE[j] = one_best_RSME
allbestModel[[j]] = one_best_Model
allpredictedMethod[groupj] = one_best_Type
allpredictedMSE[groupj] = one_best_RSME
if (one_best_Type == "ELASTICNET") {
ENETLAMBDA = one_best_Pars[[1]]$lambda
allpredictedCV[groupj]  = predict(one_best_Model,newx=validx,s=ENETLAMBDA)
} else if (one_best_Type == "ROBUST") {
allpredictedCV[groupj] = predict(one_best_Model,newdata=validdata)
}
}
})
paste("Elapsed times: ", timing["elapsed"])
stopCluster(cls)
########################################
### Print Best Model of Each Fold
########################################
allbestTypes
for (j in 1:nfolds) {
writemodel = paste("The best model at fold", j,
"is of type", allbestTypes[j],
", its RMSE", round(allbestRMSE[j],5),
"with parameter(s)",allbestPars[[j]])
print(writemodel, quote = FALSE)
}
########################################
### The best model overall
########################################
BestTypeOverall = allbestTypes[which.min(allbestRMSE)]
BestModelOverall = allbestModel[which.min(allbestRMSE)]
BestParamOverall = allbestPars[which.min(allbestRMSE)]
BestRMSEOverall = allbestRMSE[which.min(allbestRMSE)]
print(paste("Best Model: ", BestTypeOverall), quote = FALSE)
print(paste("Best Model RMSE: ", BestRMSEOverall), quote = FALSE)
print(BestParamOverall[[1]], quote = FALSE)
if (BestTypeOverall == "ELASTICNET") {
bestcoef = coef(BestModelOverall[[1]], s = one_best_Pars[[1]]$lambda)
} else if (BestTypeOverall == "ROBUST") {
bestcoef = coef(BestModelOverall[[1]])
}
options(scipen=999)
round(bestcoef,4)
# Best Model RMSE for each fold
plot(c(1:nfolds),allbestRMSE, main = "Best RMSE of Each Fold",
xaxt="n", type = "l", xlab = "", ylab = "Best RMSE", pch = 19)
abline(v = which.min(allbestRMSE), col = "red", lwd = 2, lty = 2)
points(allbestRMSE ~ c(1:nfolds), pch = 20, cex = 2)
axis(side = 1, at = 1:length(allbestTypes), labels = allbestTypes, las = 2)
text(x = c(1:nfolds), y = allbestRMSE, labels = round(allbestRMSE, 5), pos = 3, offset = 0.5)
########################################
### Outer model assessments
########################################
y = myAuto$logRetailprice
# Actual VS Predicted Retail Price plot
plot(exp(y)~exp(allpredictedCV), main = "Actual vs Predicted Retail Price in USD",
xlab="Predicted Retail Price in USD", ylab="Actual Retail Price in USD")
abline(a = 0, b = 1, col = "blue", lwd = 2)
abline(v = 40000, col = "red", lwd = 2, lty = 2)
# get CV assessment
CV.assess = mean((allpredictedCV-y)^2)
print(paste("CV assessment: ", CV.assess), quote = FALSE)
# get RMSE assessment
RMSE = sqrt(mean((allpredictedCV-y)^2))
print(paste("RMSE assessment: ", RMSE), quote = FALSE)
# get R2 assessment
R2 = 1-sum((allpredictedCV-y)^2)/sum((y-mean(y))^2)
print(paste("R2 assessment: ", R2), quote = FALSE)
# MAE
MAE = mean(abs(exp(allpredictedCV)-exp(y)))
print(paste("Predicted retail price is off +/- USD ",round(MAE,2)), quote = FALSE)
pct_err.median = round((MAE/summary(exp(y))[3])*100,2)
print(paste("Percent error: ",pct_err.median," from median retail price"), quote = FALSE)
pct_err.mean = round((MAE/summary(exp(y))[4])*100,2)
print(paste("Percent error: ",pct_err.mean," from mean retail price"), quote = FALSE)
print(paste("The best model",one_best_Type))
set.seed(8)
fit_rlm = rlm(Model.Robust, data=myAuto.reduced, psi=psi.hampel)
bestcoef =coef(fit_rlm)
options(scipen=999)
round(bestcoef,4)
imp <- as.data.frame(varImp(fit_rlm))
imp <- data.frame(overall = imp$Overall,
names   = rownames(imp))
imp[order(imp$overall,decreasing = T),]
registerDoSEQ()
82*5
# Set path
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/Midterm Project")
# Read in libraries
library(dplyr)
library(glmnet)
library(caret)
library(MASS)
library(corrplot)
library(RColorBrewer)
library(ggformula)
# Set options
options(scipen = 999)
# Read in data
cars04 <- read.csv("04cars.csv")
# Clean the data
cars04 <- cars04 %>%
dplyr::select(-c(Length, Height)) # to preserve Pickup Type
cars04 <- na.omit(cars04)
# Transform the data
cars04 <- cars04 %>%
mutate(
AWD = if_else(Type == "AWD", 1, 0),
RWD = if_else(Type == "RWD", 1, 0),
Sport = as.factor(Sport),
SUV = as.factor(SUV),
Wagon = as.factor(Wagon),
Minivan = as.factor(Minivan),
Pickup = as.factor(Pickup),
logRetailPrice = log(Retailprice), logHorsePower = log(Horsepower),
logCityMPG = log(CityMPG), logHwyMPG = log(HwyMPG)) %>%
mutate(Type = case_when(Sport == 1 ~ "Sport",
SUV == 1 ~ "SUV",
Wagon == 1 ~ "Wagon",
Minivan == 1 ~ "Minivan",
Pickup == 1 ~ "Pickup",
TRUE ~ "Other")) %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
full.model.data <- cars04 %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
### Data Assessment ###
# Check Normality, Transform if needed
#1
par(mfrow=c(2,2))
hist(cars04$Retailprice)
qqnorm(cars04$Retailprice); qqline(cars04$Retailprice)
#2
#par(mfrow=c(2,1))
hist(log(cars04$Retailprice)) # like this one better
qqnorm(log(cars04$Retailprice)); qqline(log(cars04$Retailprice))
#3
#par(mfrow=c(2,1))
hist(cars04$Engine)
qqnorm(cars04$Engine); qqline(cars04$Engine)
#4
#par(mfrow=c(2,1))
hist(cars04$Horsepower)
qqnorm(cars04$Horsepower); qqline(cars04$Horsepower)
#5
#par(mfrow=c(2,1))
hist(log(cars04$Horsepower)) # log transform for horsepower is better
qqnorm(log(cars04$Horsepower)); qqline(log(cars04$Horsepower))
#6
#par(mfrow=c(2,1))
hist(cars04$CityMPG)
qqnorm(cars04$CityMPG); qqline(cars04$CityMPG)
#7
hist(log(cars04$CityMPG))
qqnorm(log(cars04$CityMPG)); qqline(log(cars04$CityMPG))
#8
#par(mfrow=c(2,1))
hist(cars04$HwyMPG)
qqnorm(cars04$HwyMPG); qqline(cars04$HwyMPG)
#9
hist(log(cars04$HwyMPG))
qqnorm(log(cars04$HwyMPG)); qqline(log(cars04$HwyMPG))
#10
#par(mfrow=c(2,1))
hist(cars04$Weight)
qqnorm(cars04$Weight); qqline(cars04$Weight)
#11
#par(mfrow=c(2,1))
hist(cars04$Wheelbase)
qqnorm(cars04$Wheelbase); qqline(cars04$Wheelbase)
# Check correlations
par(mfrow = c(1,1))
cars04cor <- cor(select_if(cars04, is.numeric), use = "pairwise.complete.obs")
corrplot(cars04cor, type = "upper", order = "hclust",
col = rev(brewer.pal(n = 8, name = "RdYlBu")))
# Check for interactions
ggplot(data = cars04, aes(x = Horsepower, y = Retailprice, color = Type)) +
geom_point() +
geom_smooth(se = FALSE) +
labs(title = "Retailprice as a function of Horsepower")
ggplot(data = cars04, aes(x = Cylinders, y = Retailprice, color = Type)) +
geom_point()
ggplot(aes(x = Type, y = Retailprice, fill = Type)) +
geom_violin() +
labs(title = "Retailprice density as a function of Type")
# Set path
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/Midterm Project")
# Read in libraries
library(dplyr)
library(glmnet)
library(caret)
library(MASS)
library(corrplot)
library(RColorBrewer)
library(ggformula)
# Set options
options(scipen = 999)
# Read in data
cars04 <- read.csv("04cars.csv")
# Clean the data
cars04 <- cars04 %>%
dplyr::select(-c(Length, Height)) # to preserve Pickup Type
cars04 <- na.omit(cars04)
# Transform the data
cars04 <- cars04 %>%
mutate(
AWD = if_else(Type == "AWD", 1, 0),
RWD = if_else(Type == "RWD", 1, 0),
Sport = as.factor(Sport),
SUV = as.factor(SUV),
Wagon = as.factor(Wagon),
Minivan = as.factor(Minivan),
Pickup = as.factor(Pickup),
logRetailPrice = log(Retailprice), logHorsePower = log(Horsepower),
logCityMPG = log(CityMPG), logHwyMPG = log(HwyMPG)) %>%
mutate(Type = case_when(Sport == 1 ~ "Sport",
SUV == 1 ~ "SUV",
Wagon == 1 ~ "Wagon",
Minivan == 1 ~ "Minivan",
Pickup == 1 ~ "Pickup",
TRUE ~ "Other")) %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
full.model.data <- cars04 %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
### Data Assessment ###
# Check Normality, Transform if needed
#1
par(mfrow=c(2,2))
hist(cars04$Retailprice)
qqnorm(cars04$Retailprice); qqline(cars04$Retailprice)
#2
#par(mfrow=c(2,1))
hist(log(cars04$Retailprice)) # like this one better
qqnorm(log(cars04$Retailprice)); qqline(log(cars04$Retailprice))
#3
#par(mfrow=c(2,1))
hist(cars04$Engine)
qqnorm(cars04$Engine); qqline(cars04$Engine)
#4
#par(mfrow=c(2,1))
hist(cars04$Horsepower)
qqnorm(cars04$Horsepower); qqline(cars04$Horsepower)
#5
#par(mfrow=c(2,1))
hist(log(cars04$Horsepower)) # log transform for horsepower is better
qqnorm(log(cars04$Horsepower)); qqline(log(cars04$Horsepower))
#6
#par(mfrow=c(2,1))
hist(cars04$CityMPG)
qqnorm(cars04$CityMPG); qqline(cars04$CityMPG)
#7
hist(log(cars04$CityMPG))
qqnorm(log(cars04$CityMPG)); qqline(log(cars04$CityMPG))
#8
#par(mfrow=c(2,1))
hist(cars04$HwyMPG)
qqnorm(cars04$HwyMPG); qqline(cars04$HwyMPG)
#9
hist(log(cars04$HwyMPG))
qqnorm(log(cars04$HwyMPG)); qqline(log(cars04$HwyMPG))
#10
#par(mfrow=c(2,1))
hist(cars04$Weight)
qqnorm(cars04$Weight); qqline(cars04$Weight)
#11
#par(mfrow=c(2,1))
hist(cars04$Wheelbase)
qqnorm(cars04$Wheelbase); qqline(cars04$Wheelbase)
# Check correlations
par(mfrow = c(1,1))
cars04cor <- cor(select_if(cars04, is.numeric), use = "pairwise.complete.obs")
corrplot(cars04cor, type = "upper", order = "hclust",
col = rev(brewer.pal(n = 8, name = "RdYlBu")))
# Check for interactions
ggplot(data = cars04, aes(x = Horsepower, y = Retailprice, color = Type)) +
geom_point() +
geom_smooth(se = FALSE) +
labs(title = "Retailprice as a function of Horsepower")
ggplot(data = cars04, aes(x = Cylinders, y = Retailprice, color = Type)) +
geom_point()
ggplot(data = cars04, aes(x = Type, y = Retailprice, fill = Type)) +
geom_violin() +
labs(title = "Retailprice density as a function of Type")
# ### Feature Selection ###
#
#
# lambdalist = seq(0.001, 1, length = 100)
# alphalist = seq(0.1, 1, length = 100)
# model.data <- cars04 %>%
#   dplyr::select(logRetailPrice, Cylinders, logHorsePower, Type, AWD, RWD)
#
# ENET.model <- (logRetailPrice ~ .)
# fit_caret_ENET = train(ENET.model, data = model.data,
#                        method = "glmnet", trControl = training,
#                        tuneGrid = expand.grid(alpha = alphalist,
#                                               lambda = lambdalist),
#                        na.action = na.omit)
#fit_caret_ENET
#coef(fit_caret_ENET$finalModel, s = fit_caret_ENET$bestTune$lambda)
### Results ###
# Based on repeated runs of the ENET on a set seed, removing useless
# predictors on each run, I can trim the model
# to logRetailPrice, Cylinders, logHorsePower, Type, AWD, and RWD
##### model assessment OUTER 5-fold CV #####
##### (with model selection INNER 10-fold CV as part of model-fitting) #####
carsmatrix = model.matrix(logRetailPrice ~., data = model.data)
fulldata = data.frame(logRetailPrice = model.data$logRetailPrice, carsmatrix)
n = dim(fulldata)[1]
lambdalist = seq(0.001, 1, length = 100)
alphalist = seq(0.01, 1, length = 100)
# define the cross-validation splits
nfolds = 5
groups.out = rep(1:nfolds,length = n)
set.seed(8)
cvgroups.out = sample(groups.out, n)
allpredictedCV.out = rep(NA, n)
# define the models
ENET.model = (logRetailPrice ~ .)
RR.model = (logRetailPrice ~ .)
# define the place holders
allpred.CV = rep(NA,n)
allpred.Method = rep(NA,n)
allpred.RMSE = rep(NA,n)
all_best_RMSE = rep(NA,nfolds)
all_best_Types = rep(NA,nfolds)
all_best_Pars = vector("list",nfolds)
all_best_Model = vector("list",nfolds)
### model assessment OUTER shell ###
for (ii in 1:nfolds)  {
groupii = (cvgroups.out == ii)
# define the training set for outer loop
train = fulldata[!groupii,]
dummyTrain = dummyVars(" ~. ", data = train)
trainx = model.matrix(logRetailPrice ~ ., data = train)[,-1]
#dummTrainX = dummyVars(" ~. ", data = trainx)
trainy = train$logRetailPrice
dummyTrainY = dummyVars(" ~. ", data = trainy)
#define the validation set for outer loop
test = fulldata[groupii,]
dummyTest = dummyVars(" ~. ", data = test)
testx = model.matrix(logRetailPrice ~ ., data = test)[,-1]
#dummyTestX = dummyVars(" ~. ", data = testx)
# training controls for ENET
training = trainControl(method = "cv", number = 10)
dataused = train
# model selection
# cross-validation of ENET model
fit_caret_ENET = train(ENET.model,
data = dataused,
method = "glmnet",
trControl = training,
tuneGrid = expand.grid(alpha = alphalist,
lambda = lambdalist))
# fit_caret_ENET.full = train(ENET.model,
#                        data = dataused.full,
#                        method = "glmnet",
#                        trControl = training,
#                        tuneGrid = expand.grid(alpha = alphalist,
#                                               lambda = lambdalist))
#
# cross-validation of RR model
fit_caret_RR = train(RR.model,
data = dataused,
method = "rlm",
maxit = 50,
trControl = training)
# fit_caret_RR.full = train(RR.model,
#                      data = dataused.full,
#                      method = "rlm",
#                      trControl = training)
#
# All best models
all_best_Types = c("ENET","RR")
all_best_Pars = list(fit_caret_ENET$bestTune, fit_caret_RR$bestTune)
all_best_Models = list(glmnet(trainx, trainy,
alpha=fit_caret_ENET$bestTune$alpha,
lambda=lambdalist),
fit_caret_RR$finalModel)
all_best_RMSE = c(min(fit_caret_ENET$results$RMSE),
min(fit_caret_RR$results$RMSE))
# Best model, each fold
one_best_Type = all_best_Types[which.min(all_best_RMSE)]
one_best_Pars = all_best_Pars[which.min(all_best_RMSE)]
one_best_Model = all_best_Models[[which.min(all_best_RMSE)]]
one_best_RSME = all_best_RMSE[[which.min(all_best_RMSE)]]
allpred.Method[ii] = one_best_Type
allpred.RMSE[ii] = one_best_RSME
if(one_best_Type == "ENET"){
ENETlambda = one_best_Pars[[1]]$lambda
allpred.CV[groupii] = predict(one_best_Model, newx = testx, s = ENETlambda)
} else if(one_best_Type == "RR"){
RRlambda = one_best_Pars[[1]]$lambda
allpred.CV[groupii] = predict(one_best_Model, newdata = test)
}
############# END ##############
}
coef(fit_caret_RR$finalModel, s = fit_caret_RR$bestTune$lambda)
one_best_Type
one_best_Pars
one_best_RSME
y = cars04$logRetailprice
# Actual VS Predicted Retail Price plot
plot(exp(y)~exp(allpred.CV), main = "Actual vs Predicted Retail Price in USD",
xlab="Predicted Retail Price in USD", ylab="Actual Retail Price in USD")
# Actual VS Predicted Retail Price plot
plot(exp(y)~exp(allpred.CV))
y = cars04$logRetailprice
cars04$logRetailPrice
y = cars04$logRetailprice
y <- cars04$logRetailprice
y
cars04$logRetailPrice
