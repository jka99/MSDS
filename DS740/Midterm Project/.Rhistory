geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Bike Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1050, aes(label = ..adj.rr.label..)) +
facet_wrap(~Gender.20)
#Run
ggplot(aes(x = RunTime.20, y = OverallTime.21), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(legend.position = "none") +
labs(x = "2020 Run Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..))
ggplot(aes(x = RunTime.20, y = OverallTime.21, color = Gender.20), data = IM_20_21) +
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Run Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1050, aes(label = ..adj.rr.label..)) +
facet_wrap(~Gender.20)
#Create a forest plot that shows the estimated slope parameter and associated confidence interval for each 2020 event time component (swim, bike, run, transition).
df <- data.frame(characteristic = c("SwimTime.20", "BikeTime.20", "RunTime.20", "TransTime.20"),
coefficient = c(5.963, 2.059, 1.4449, 8.979),
Lower = c(4.837, 1.771, 1.238, 7.253),
Upper = c(7.09, 2.348, 1.66, 10.705),
p_value = c("<0.001", "<0.001", "<0.001", "<0.001"),
index = c(1,2,3,4))
ggplot(data = df, aes(y=characteristic, x=coefficient, xmin=Lower, xmax = Upper)) +
geom_point() +
geom_errorbarh(height=0.1)
#Fit a multiple regression model that small and interpretable, but still fits the data well. This will likely take some iteration. Do not show every step that you take in this process. Instead, narrate broadly the process that you followed and share details on the final model that you chose.
mlm_model.1 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20)
mlm_model.2 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 * BikeTime.20 * RunTime.20 * TransTime.20)
mlm_model.3 <- lm(data = IM_20_21, OverallTime.21 ~ (SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20)^2)
mlm_model.4 <- lm(data = IM_20_21, OverallTime.21 ~ (SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20)^4)
mlm_model.5 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20 + SwimTime.20:BikeTime.20 + SwimTime.20:RunTime.20)
mlm_model.6 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + TransTime.20 + SwimTime.20:BikeTime.20)
mlm_model.7 <- lm(data = IM_20_21, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + SwimTime.20:BikeTime.20)
mlm_null <- lm(data = IM_20_21, OverallTime.21 ~ 1)
# step(mlm_model.1, scope = list(lower = mlm_null, upper = mlm_model.1), direction = "both")
# summary(mlm_model.4)
# anova(mlm_model.6)
# summary(mlm_model.6)
#summary(mlm_model.2)
tidy(mlm_model.1)
tidy(mlm_model.2)
tidy(mlm_model.3)
tidy(mlm_model.4)
tidy(mlm_model.5)
tidy(mlm_model.6)
tidy(mlm_model.7)
glance(mlm_model.1)
glance(mlm_model.2)
glance(mlm_model.3)
glance(mlm_model.4)
glance(mlm_model.5)
glance(mlm_model.6)
glance(mlm_model.7)
IM_20_21.female <- IM_20_21 %>%
filter(Gender.20 == "Female")
IM_20_21.male <- IM_20_21 %>%
filter(Gender.20 == "Male")
mlm_model.7f <- lm(data = IM_20_21.female, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + SwimTime.20:BikeTime.20)
mlm_model.7m <- lm(data = IM_20_21.male, OverallTime.21 ~ SwimTime.20 + BikeTime.20 + RunTime.20 + SwimTime.20:BikeTime.20)
summary(mlm_model.7)
summary(mlm_model.7f)
summary(mlm_model.7m)
#Use your chosen multiple regression model to address the primary research question. You may choose to report and interpret estimated model coefficients (possibly create another forest plot). You may choose to report and interpret R-squared or adjusted R-squared. Include any plots or tables that help support your explanation.
avPlots(mlm_model.7, main = "MLR Model for Ironman 2020-2021")
avPlots(mlm_model.7f, main = "MLR Model for Ironman 2020-2021: Females")
avPlots(mlm_model.7m, main = "MLR Model for Ironman 2020-2021: Males")
IM_21 = IM_21 %>% mutate(Group = case_when(
Name %in% IM_20_21$Name ~ "Repeat",
TRUE ~ "Non-repeat"))
IM_21_Repeat <- IM_21 %>%
filter(Group == "Repeat")
IM_21_Nonrepeat <- IM_21 %>%
filter(Group == "Non-repeat")
print(paste("The mean SwimTime for repeat athletes is", round(mean(IM_21_Repeat$SwimTime, na.rm = T),2)))
print(paste("The mean SwimTime for non-repeat athletes is", round(mean(IM_21_Nonrepeat$SwimTime, na.rm = T),2)))
#print(paste("The mean BikeTime for repeat athletes is",r_bike_mean))
r_bike_mean <- round(mean(IM_21_Repeat$BikeTime, na.rm = T),2)
#print(paste("The mean BikeTime for non-repeat athletes is", round(mean(IM_21_Nonrepeat$BikeTime, na.rm = T),2)))
r_swim_mean <- round(mean(IM_21_Repeat$SwimTime, na.rm = T),2)
nr_swim_mean <- round(mean(IM_21_Nonrepeat$SwimTime, na.rm = T),2)
r_bike_mean <- round(mean(IM_21_Repeat$BikeTime, na.rm = T),2)
nr_bike_mean <- round(mean(IM_21_Nonrepeat$BikeTime, na.rm = T),2)
r_run_mean <- round(mean(IM_21_Repeat$RunTime, na.rm = T),2)
nr_run_mean <- round(mean(IM_21_Nonrepeat$RunTime, na.rm = T),2)
r_overall_mean <- round(mean(IM_21_Repeat$OverallTime, na.rm = T),2)
nr_overall_mean <- round(mean(IM_21_Nonrepeat$OverallTime, na.rm = T),2)
print(paste("The mean BikeTime for repeat athletes is",r_swim_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_swim_mean),2)
print(paste("The mean BikeTime for repeat athletes is",r_bike_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_bike_mean),2)
print(paste("The mean BikeTime for repeat athletes is",r_run_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_run_mean),2)
print(paste("The mean BikeTime for repeat athletes is",r_overall_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_overall_mean),2)
ggplot(data = IM_21, aes(x = SwimTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Swim Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
ggplot(data = IM_21, aes(x = BikeTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Bike Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
ggplot(data = IM_21, aes(x = RunTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Run Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
gghistogram(IM_21, x = "OverallTime",
add = "mean", rug = F,
color = "Group", fill = "Group")
A <- 10
?mhv()
library(MVTests)
?mhz
library(MASS)  #help(qda)
library(pROC)
# for assumption-checking
library(mvnormalTest)  # of multivariate normality; or library(MVN) or (mvnTest)
library(MVTests)  # of constant covariance; or library(biotools) or (biotools) or (heplots)
# for data organization
library(dplyr)
# for visuals
library(ggformula)
?mhz
?pmax
pmin( c( 8, 1, 8 ), c(2, 7, 8) )
1- (4/3)
0.91*0.91
# Set path
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/Midterm Project")
# Read in libraries
library(dplyr)
library(glmnet)
library(caret)
library(MASS)
library(corrplot)
library(RColorBrewer)
library(ggformula)
# Set options
options(scipen = 999)
# Read in data
cars04 <- read.csv("04cars.csv")
# Clean the data
cars04 <- cars04 %>%
dplyr::select(-c(Length, Height)) # to preserve Pickup Type
cars04 <- na.omit(cars04)
# Transform the data
cars04 <- cars04 %>%
mutate(
AWD = if_else(Type == "AWD", 1, 0),
RWD = if_else(Type == "RWD", 1, 0),
Sport = as.factor(Sport),
SUV = as.factor(SUV),
Wagon = as.factor(Wagon),
Minivan = as.factor(Minivan),
Pickup = as.factor(Pickup),
logRetailPrice = log(Retailprice), logHorsePower = log(Horsepower),
logCityMPG = log(CityMPG), logHwyMPG = log(HwyMPG)) %>%
mutate(Type = case_when(Sport == 1 ~ "Sport",
SUV == 1 ~ "SUV",
Wagon == 1 ~ "Wagon",
Minivan == 1 ~ "Minivan",
Pickup == 1 ~ "Pickup",
TRUE ~ "Other")) %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
full.model.data <- cars04 %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
### Data Assessment ###
# Check Normality, Transform if needed
#1
par(mfrow=c(2,2))
hist(cars04$Retailprice)
qqnorm(cars04$Retailprice); qqline(cars04$Retailprice)
#2
#par(mfrow=c(2,1))
hist(log(cars04$Retailprice)) # like this one better
qqnorm(log(cars04$Retailprice)); qqline(log(cars04$Retailprice))
#3
#par(mfrow=c(2,1))
hist(cars04$Engine)
qqnorm(cars04$Engine); qqline(cars04$Engine)
#4
#par(mfrow=c(2,1))
hist(cars04$Horsepower)
qqnorm(cars04$Horsepower); qqline(cars04$Horsepower)
#5
#par(mfrow=c(2,1))
hist(log(cars04$Horsepower)) # log transform for horsepower is better
qqnorm(log(cars04$Horsepower)); qqline(log(cars04$Horsepower))
#6
#par(mfrow=c(2,1))
hist(cars04$CityMPG)
qqnorm(cars04$CityMPG); qqline(cars04$CityMPG)
#7
hist(log(cars04$CityMPG))
qqnorm(log(cars04$CityMPG)); qqline(log(cars04$CityMPG))
#8
#par(mfrow=c(2,1))
hist(cars04$HwyMPG)
qqnorm(cars04$HwyMPG); qqline(cars04$HwyMPG)
#9
hist(log(cars04$HwyMPG))
qqnorm(log(cars04$HwyMPG)); qqline(log(cars04$HwyMPG))
#10
#par(mfrow=c(2,1))
hist(cars04$Weight)
qqnorm(cars04$Weight); qqline(cars04$Weight)
#11
#par(mfrow=c(2,1))
hist(cars04$Wheelbase)
qqnorm(cars04$Wheelbase); qqline(cars04$Wheelbase)
# Check correlations
par(mfrow = c(1,1))
cars04cor <- cor(select_if(cars04, is.numeric), use = "pairwise.complete.obs")
corrplot(cars04cor, type = "upper", order = "hclust",
col = rev(brewer.pal(n = 8, name = "RdYlBu")))
# Check for interactions
ggplot(data = cars04, aes(x = Horsepower, y = Retailprice, color = Type)) +
geom_point() +
geom_smooth(se = FALSE) +
labs(title = "Retailprice as a function of Horsepower")
ggplot(data = cars04, aes(x = Cylinders, y = Retailprice, color = Type)) +
geom_point()
ggplot(data = cars04, aes(x = Type, y = Retailprice, fill = Type)) +
geom_jitter() +
labs(title = "Retailprice density as a function of Type")
# ### Feature Selection ###
#
#
# lambdalist = seq(0.001, 1, length = 100)
# alphalist = seq(0.1, 1, length = 100)
# model.data <- cars04 %>%
#   dplyr::select(logRetailPrice, Cylinders, logHorsePower, Type, AWD, RWD)
#
# ENET.model <- (logRetailPrice ~ .)
# fit_caret_ENET = train(ENET.model, data = model.data,
#                        method = "glmnet", trControl = training,
#                        tuneGrid = expand.grid(alpha = alphalist,
#                                               lambda = lambdalist),
#                        na.action = na.omit)
#fit_caret_ENET
#coef(fit_caret_ENET$finalModel, s = fit_caret_ENET$bestTune$lambda)
### Results ###
# Based on repeated runs of the ENET on a set seed, removing useless
# predictors on each run, I can trim the model
# to logRetailPrice, Cylinders, logHorsePower, Type, AWD, and RWD
##### model assessment OUTER 5-fold CV #####
##### (with model selection INNER 10-fold CV as part of model-fitting) #####
carsmatrix = model.matrix(logRetailPrice ~., data = model.data)
# Set path
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/Midterm Project")
# Read in libraries
library(dplyr)
library(glmnet)
library(caret)
library(MASS)
library(corrplot)
library(RColorBrewer)
library(ggformula)
# Set options
options(scipen = 999)
# Read in data
cars04 <- read.csv("04cars.csv")
# Clean the data
cars04 <- cars04 %>%
dplyr::select(-c(Length, Height)) # to preserve Pickup Type
cars04 <- na.omit(cars04)
# Transform the data
cars04 <- cars04 %>%
mutate(
AWD = if_else(Type == "AWD", 1, 0),
RWD = if_else(Type == "RWD", 1, 0),
Sport = as.factor(Sport),
SUV = as.factor(SUV),
Wagon = as.factor(Wagon),
Minivan = as.factor(Minivan),
Pickup = as.factor(Pickup),
logRetailPrice = log(Retailprice), logHorsePower = log(Horsepower),
logCityMPG = log(CityMPG), logHwyMPG = log(HwyMPG)) %>%
mutate(Type = case_when(Sport == 1 ~ "Sport",
SUV == 1 ~ "SUV",
Wagon == 1 ~ "Wagon",
Minivan == 1 ~ "Minivan",
Pickup == 1 ~ "Pickup",
TRUE ~ "Other")) %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
full.model.data <- cars04 %>%
dplyr::select(Retailprice, logRetailPrice, Engine, Cylinders, Horsepower,
logHorsePower, CityMPG, logCityMPG, HwyMPG, logHwyMPG,
Weight, Wheelbase, Type, AWD, RWD)
### Data Assessment ###
# Check Normality, Transform if needed
#1
par(mfrow=c(2,2))
hist(cars04$Retailprice)
qqnorm(cars04$Retailprice); qqline(cars04$Retailprice)
#2
#par(mfrow=c(2,1))
hist(log(cars04$Retailprice)) # like this one better
qqnorm(log(cars04$Retailprice)); qqline(log(cars04$Retailprice))
#3
#par(mfrow=c(2,1))
hist(cars04$Engine)
qqnorm(cars04$Engine); qqline(cars04$Engine)
#4
#par(mfrow=c(2,1))
hist(cars04$Horsepower)
qqnorm(cars04$Horsepower); qqline(cars04$Horsepower)
#5
#par(mfrow=c(2,1))
hist(log(cars04$Horsepower)) # log transform for horsepower is better
qqnorm(log(cars04$Horsepower)); qqline(log(cars04$Horsepower))
#6
#par(mfrow=c(2,1))
hist(cars04$CityMPG)
qqnorm(cars04$CityMPG); qqline(cars04$CityMPG)
#7
hist(log(cars04$CityMPG))
qqnorm(log(cars04$CityMPG)); qqline(log(cars04$CityMPG))
#8
#par(mfrow=c(2,1))
hist(cars04$HwyMPG)
qqnorm(cars04$HwyMPG); qqline(cars04$HwyMPG)
#9
hist(log(cars04$HwyMPG))
qqnorm(log(cars04$HwyMPG)); qqline(log(cars04$HwyMPG))
#10
#par(mfrow=c(2,1))
hist(cars04$Weight)
qqnorm(cars04$Weight); qqline(cars04$Weight)
#11
#par(mfrow=c(2,1))
hist(cars04$Wheelbase)
qqnorm(cars04$Wheelbase); qqline(cars04$Wheelbase)
# Check correlations
par(mfrow = c(1,1))
cars04cor <- cor(select_if(cars04, is.numeric), use = "pairwise.complete.obs")
corrplot(cars04cor, type = "upper", order = "hclust",
col = rev(brewer.pal(n = 8, name = "RdYlBu")))
# Check for interactions
ggplot(data = cars04, aes(x = Horsepower, y = Retailprice, color = Type)) +
geom_point() +
geom_smooth(se = FALSE) +
labs(title = "Retailprice as a function of Horsepower")
ggplot(data = cars04, aes(x = Cylinders, y = Retailprice, color = Type)) +
geom_point()
ggplot(data = cars04, aes(x = Type, y = Retailprice, fill = Type)) +
geom_jitter() +
labs(title = "Retailprice density as a function of Type")
# ### Feature Selection ###
#
#
# lambdalist = seq(0.001, 1, length = 100)
# alphalist = seq(0.1, 1, length = 100)
# model.data <- cars04 %>%
#   dplyr::select(logRetailPrice, Cylinders, logHorsePower, Type, AWD, RWD)
#
# ENET.model <- (logRetailPrice ~ .)
# fit_caret_ENET = train(ENET.model, data = model.data,
#                        method = "glmnet", trControl = training,
#                        tuneGrid = expand.grid(alpha = alphalist,
#                                               lambda = lambdalist),
#                        na.action = na.omit)
#fit_caret_ENET
#coef(fit_caret_ENET$finalModel, s = fit_caret_ENET$bestTune$lambda)
### Results ###
# Based on repeated runs of the ENET on a set seed, removing useless
# predictors on each run, I can trim the model
# to logRetailPrice, Cylinders, logHorsePower, Type, AWD, and RWD
##### model assessment OUTER 5-fold CV #####
##### (with model selection INNER 10-fold CV as part of model-fitting) #####
model.data <- cars04 %>%
dplyr::select(logRetailPrice, Cylinders, logHorsePower, Type, AWD, RWD)
carsmatrix = model.matrix(logRetailPrice ~., data = model.data)
fulldata = data.frame(logRetailPrice = model.data$logRetailPrice, carsmatrix)
n = dim(fulldata)[1]
lambdalist = seq(0.001, 1, length = 100)
alphalist = seq(0.01, 1, length = 100)
# define the cross-validation splits
nfolds = 5
groups.out = rep(1:nfolds,length = n)
set.seed(8)
cvgroups.out = sample(groups.out, n)
allpredictedCV.out = rep(NA, n)
# define the models
ENET.model = (logRetailPrice ~ .)
RR.model = (logRetailPrice ~ .)
# define the place holders
allpred.CV = rep(NA,n)
allpred.Method = rep(NA,n)
allpred.RMSE = rep(NA,n)
all_best_RMSE = rep(NA,nfolds)
all_best_Types = rep(NA,nfolds)
all_best_Pars = vector("list",nfolds)
all_best_Model = vector("list",nfolds)
### model assessment OUTER shell ###
for (ii in 1:nfolds)  {
groupii = (cvgroups.out == ii)
# define the training set for outer loop
train = fulldata[!groupii,]
dummyTrain = dummyVars(" ~. ", data = train)
trainx = model.matrix(logRetailPrice ~ ., data = train)[,-1]
#dummTrainX = dummyVars(" ~. ", data = trainx)
trainy = train$logRetailPrice
dummyTrainY = dummyVars(" ~. ", data = trainy)
#define the validation set for outer loop
test = fulldata[groupii,]
dummyTest = dummyVars(" ~. ", data = test)
testx = model.matrix(logRetailPrice ~ ., data = test)[,-1]
#dummyTestX = dummyVars(" ~. ", data = testx)
# training controls for ENET
training = trainControl(method = "cv", number = 10)
dataused = train
# model selection
# cross-validation of ENET model
fit_caret_ENET = train(ENET.model,
data = dataused,
method = "glmnet",
trControl = training,
tuneGrid = expand.grid(alpha = alphalist,
lambda = lambdalist))
# fit_caret_ENET.full = train(ENET.model,
#                        data = dataused.full,
#                        method = "glmnet",
#                        trControl = training,
#                        tuneGrid = expand.grid(alpha = alphalist,
#                                               lambda = lambdalist))
#
# cross-validation of RR model
fit_caret_RR = train(RR.model,
data = dataused,
method = "rlm",
maxit = 50,
trControl = training)
# fit_caret_RR.full = train(RR.model,
#                      data = dataused.full,
#                      method = "rlm",
#                      trControl = training)
#
# All best models
all_best_Types = c("ENET","RR")
all_best_Pars = list(fit_caret_ENET$bestTune, fit_caret_RR$bestTune)
all_best_Models = list(glmnet(trainx, trainy,
alpha=fit_caret_ENET$bestTune$alpha,
lambda=lambdalist),
fit_caret_RR$finalModel)
all_best_RMSE = c(min(fit_caret_ENET$results$RMSE),
min(fit_caret_RR$results$RMSE))
# Best model, each fold
one_best_Type = all_best_Types[which.min(all_best_RMSE)]
one_best_Pars = all_best_Pars[which.min(all_best_RMSE)]
one_best_Model = all_best_Models[[which.min(all_best_RMSE)]]
one_best_RSME = all_best_RMSE[[which.min(all_best_RMSE)]]
allpred.Method[ii] = one_best_Type
allpred.RMSE[ii] = one_best_RSME
if(one_best_Type == "ENET"){
ENETlambda = one_best_Pars[[1]]$lambda
allpred.CV[groupii] = predict(one_best_Model, newx = testx, s = ENETlambda)
} else if(one_best_Type == "RR"){
RRlambda = one_best_Pars[[1]]$lambda
allpred.CV[groupii] = predict(one_best_Model, newdata = test)
}
############# END ##############
}
coef(fit_caret_RR$finalModel, s = fit_caret_RR$bestTune$lambda)
one_best_Type
one_best_Pars
one_best_RSME
y <- cars04$logRetailprice
# Actual VS Predicted Retail Price plot
plot(exp(y)~exp(allpred.CV))
y <- cars04$logRetailprice
cars05 <- cars04
cars05.1 <- cars04$logRetailPrice
var <- cars04$logRetailprice
var <- cars04$logRetailprice
cars05.2 <- cars04$logRetailprice
cars04$logRetailprice
cars04$logRetailPrice
var <- cars04$logRetailprice
var
cars05.2 <- cars04$logRetailprice
ggplot(data = cars04, aes(x = Type, y = Retailprice, color = Type)) +
geom_jitter() +
labs(title = "Retailprice density as a function of Type")
ggplot(data = cars04, aes(x = Cylinders, y = logRetailPrice, color = Type)) +
geom_point()
ggplot(data = cars04, aes(x = Type, y = logRetailPrice, color = Type)) +
geom_jitter() +
labs(title = "Retailprice density as a function of Type")
ggplot(data = cars04, aes(x = Type, y = logRetailPrice, color = Type)) +
geom_boxplot()
ggplot(data = cars04, aes(x = Type, y = logRetailPrice, fill = Type)) +
geom_boxplot()
