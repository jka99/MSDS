yint_lower = (sd(y) * (b-1)/w[2]) +
(w[1]/w[2] * mean(x) * sd.ratio) + mean(y)
my_iris %>%
gf_point(Petal.Length ~ Sepal.Length,
color =~ Species,
pch =~ Species) %>%
gf_abline(intercept = yint, slope = my_slope) %>%
gf_abline(intercept = yint_lower, slope = my_slope,
lty = 2) %>%
gf_abline(intercept = yint_upper, slope = my_slope,
lty = 2)
head(predict(fit_iris))
conf_mat = table(predict(fit_iris), my_iris$Species)
conf_mat
# Accuracy
sum(diag(conf_mat)) / dim(my_iris)[1]  # (48+46) / 100
fit_iris$results[3, ]
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggformula)
library(kernlab)
library(caret)
n = 100
set.seed(516)
poly_example = data.frame(x1 = runif(n, -1, 1),
x2 = runif(n, -1, 1))
poly_example <- poly_example %>%
mutate(y = factor(ifelse(.5 - 3*x1^2 + x2 > 0, 1, -1)))
polynomial = data.frame(x_poly = seq(-1, 1, by = .01))
polynomial <- polynomial %>%
mutate(y_poly = -.5 + 3*x_poly^2)
gf_line(y_poly ~ x_poly, data = polynomial) %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y, data = poly_example) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")),
coord_cartesian(ylim = c(-1,1)))
poly_example <- poly_example %>%
mutate(x1_squared = x1^2)
# x_1 squared can't be negative, but we'll include some negative values when plotting the line to make the line look more complete
linear = data.frame(x1_squared = seq(-.25, 1, by = .01))
linear <- linear %>%
mutate(y_linear = -.5 + 3*x1_squared)
gf_line(y_linear ~ x1_squared, data = linear) %>%
gf_point(x2 ~ x1_squared, color =~ y, pch =~ y,
data = poly_example) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")),
coord_cartesian(xlim = c(0,1), ylim = c(-1,1)))
set.seed(516)
data_used = poly_example
ctrl = trainControl(method = "cv", number = 10)
fit_poly = train(y ~ x1 + x2,
data = data_used,
method = "svmPoly",
tuneGrid = expand.grid(C = c(1),
degree = c(1,2,3),
scale = c(1)),  # gamma
preProcess = c("center","scale"),
trControl = ctrl)
fit_poly
n = 100
set.seed(515)
radial_example = data.frame(x1 = runif(n, -1, 1),
x2 = runif(n, -1, 1))
radial_example <- radial_example %>%
tibble::rownames_to_column("Row") %>%
mutate(y = factor(ifelse(-.75 + 2*x1^2 + x2^2 > 0, 1, -1)),
scale_x1 = scale(x1),
scale_x2 = scale(x2))
radial_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
set.seed(515)
data_used = radial_example
ctrl = trainControl(method = "cv", number = 10)
fit_radial = train(y ~ x1 + x2,
data = data_used,
method = "svmRadial",
tuneGrid = expand.grid(C = c(.001, .01, .1, 1, 5, 10, 100),
sigma = c(0.5, 1, 2, 3, 4)),
preProcess = c("center","scale"),
prob.model = TRUE,
trControl = ctrl)
fit_radial
xgrid = expand.grid(x1 = seq(-1, 1, by = .05),
x2 = seq(-1, 1, by = .05))
preds = predict(fit_radial, newdata = xgrid, type = "prob")
head(preds)
xgrid <- xgrid %>%
mutate(prob_classA = preds[ ,1])
head(xgrid)
ggplot(xgrid, aes(x1, x2, z = prob_classA)) +
geom_contour(breaks = .5) +
geom_point(aes(x1, x2, shape = y, color = y),
data = radial_example,
inherit.aes = FALSE) +
scale_shape_manual(values = c(4,2)) +
scale_color_manual(values = c("red", "blue"))
gf_contour(prob_classA ~ x1 + x2, breaks = .5, data = xgrid) # %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y, data = radial_example)
n = 20
set.seed(517)
linear_example = data.frame(x1 = runif(n, 0, 1),
x2 = runif(n, 0, 1))
linear_example <- linear_example %>%
mutate(y = factor(ifelse(-1 - 2*x1 + 3*x2 > 0, 1, -1)))
linear_example$y[14] = 1 # making the example non-separable
linear_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y, lwd = 2.5) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
n = 19
set.seed(524)
outlier_example = data.frame(x1 = c(2, runif(n, 0, 1)),
x2 = c(-1, runif(n, 0, 1)))
outlier_example <- outlier_example %>%
mutate(y = factor(ifelse(-.8 - 2*x1 + 3*x2 > 0, 1, -1)))
outlier_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
n = 20
set.seed(524)
logistic_example = data.frame(x1 = c(runif(n, 0, 1), runif(n, 0.15, 1.15)),
x2 = c(runif(n, 0, 1), runif(n, 0.15, 1.15)),
y = factor(rep(c(-1, 1), each = n)))
logistic_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggformula)
library(kernlab)
library(caret)
n = 100
set.seed(516)
poly_example = data.frame(x1 = runif(n, -1, 1),
x2 = runif(n, -1, 1))
poly_example <- poly_example %>%
mutate(y = factor(ifelse(.5 - 3*x1^2 + x2 > 0, 1, -1)))
polynomial = data.frame(x_poly = seq(-1, 1, by = .01))
polynomial <- polynomial %>%
mutate(y_poly = -.5 + 3*x_poly^2)
gf_line(y_poly ~ x_poly, data = polynomial) %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y, data = poly_example) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")),
coord_cartesian(ylim = c(-1,1)))
poly_example <- poly_example %>%
mutate(x1_squared = x1^2)
# x_1 squared can't be negative, but we'll include some negative values when plotting the line to make the line look more complete
linear = data.frame(x1_squared = seq(-.25, 1, by = .01))
linear <- linear %>%
mutate(y_linear = -.5 + 3*x1_squared)
gf_line(y_linear ~ x1_squared, data = linear) %>%
gf_point(x2 ~ x1_squared, color =~ y, pch =~ y,
data = poly_example) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")),
coord_cartesian(xlim = c(0,1), ylim = c(-1,1)))
set.seed(516)
data_used = poly_example
ctrl = trainControl(method = "cv", number = 10)
fit_poly = train(y ~ x1 + x2,
data = data_used,
method = "svmPoly",
tuneGrid = expand.grid(C = c(1),
degree = c(1,2,3),
scale = c(1)),  # gamma
preProcess = c("center","scale"),
trControl = ctrl)
fit_poly
n = 100
set.seed(515)
radial_example = data.frame(x1 = runif(n, -1, 1),
x2 = runif(n, -1, 1))
radial_example <- radial_example %>%
tibble::rownames_to_column("Row") %>%
mutate(y = factor(ifelse(-.75 + 2*x1^2 + x2^2 > 0, 1, -1)),
scale_x1 = scale(x1),
scale_x2 = scale(x2))
radial_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
set.seed(515)
data_used = radial_example
ctrl = trainControl(method = "cv", number = 10)
fit_radial = train(y ~ x1 + x2,
data = data_used,
method = "svmRadial",
tuneGrid = expand.grid(C = c(.001, .01, .1, 1, 5, 10, 100),
sigma = c(0.5, 1, 2, 3, 4)),
preProcess = c("center","scale"),
prob.model = TRUE,
trControl = ctrl)
fit_radial
xgrid = expand.grid(x1 = seq(-1, 1, by = .05),
x2 = seq(-1, 1, by = .05))
preds = predict(fit_radial, newdata = xgrid, type = "prob")
head(preds)
xgrid <- xgrid %>%
mutate(prob_classA = preds[ ,1])
head(xgrid)
ggplot(xgrid, aes(x1, x2, z = prob_classA)) +
geom_contour(breaks = .5) +
geom_point(aes(x1, x2, shape = y, color = y),
data = radial_example,
inherit.aes = FALSE) +
scale_shape_manual(values = c(4,2)) +
scale_color_manual(values = c("red", "blue"))
gf_contour(prob_classA ~ x1 + x2, breaks = .5, data = xgrid) # %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y, data = radial_example)
n = 20
set.seed(517)
linear_example = data.frame(x1 = runif(n, 0, 1),
x2 = runif(n, 0, 1))
linear_example <- linear_example %>%
mutate(y = factor(ifelse(-1 - 2*x1 + 3*x2 > 0, 1, -1)))
linear_example$y[14] = 1 # making the example non-separable
linear_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y, lwd = 2.5) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
n = 19
set.seed(524)
outlier_example = data.frame(x1 = c(2, runif(n, 0, 1)),
x2 = c(-1, runif(n, 0, 1)))
outlier_example <- outlier_example %>%
mutate(y = factor(ifelse(-.8 - 2*x1 + 3*x2 > 0, 1, -1)))
outlier_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
n = 20
set.seed(524)
logistic_example = data.frame(x1 = c(runif(n, 0, 1), runif(n, 0.15, 1.15)),
x2 = c(runif(n, 0, 1), runif(n, 0.15, 1.15)),
y = factor(rep(c(-1, 1), each = n)))
logistic_example %>%
gf_point(x2 ~ x1, color =~ y, pch =~ y) %>%
gf_refine(scale_shape_manual(values = c(4,2)),
scale_color_manual(values = c("red", "blue")))
my_iris %>%
filter(is_SV) %>%
gf_point(Petal.Length ~ Sepal.Length,
pch =~Species, size = 2.5) %>%
gf_point(Petal.Length ~ Sepal.Length,
color =~ Species,
pch =~ Species, data = my_iris) %>%
gf_labs(title = "Highlighted points are support vectors")
w = colSums(coefs * iris_SV) # beta_1, ... beta_p
w
coef
coefs
View(iris_SV)
head(coefs)
head(predict(fit_iris))
install.packages("nnet")
install.packages("nnet")
install.packages("NeuralNetTools")
install.packages("reshape2")
install.packages("reshape2")
install.packages("reshape2")
install.packages("reshape2")
install.packages("reshape2")
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/Final Project")
# Initiate Packages
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(ggplot2)
library(caret)
library(nnet)
library(NeuralNetTools)
library(ggformula)
library(reshape2)
# Read in data and combine sets
air.travel.1 <- read.csv("air_travel_test.csv")
air.travel.2 <- read.csv("air_travel_train.csv")
air.travel <- rbind(air.travel.1,air.travel.2)
# Remove NAs
air.travel.data <- na.omit(air.travel)
# Remove id Column
air.travel.data <- air.travel.data %>%
select(-id)
ggplot(data = air.travel.data, aes(x = satisfaction, fill = Class)) +
geom_bar(position = "dodge") +
labs(title = "Level of Satisfaction by Class")
ggplot(data = air.travel.data, aes(x = satisfaction, fill = Class)) +
geom_boxplot() +
labs(title = "Level of Satisfaction by Class")
ggplot(data = air.travel.data, aes(x = satisfaction, fill = Class)) +
geom_violin() +
labs(title = "Level of Satisfaction by Class")
ggplot(data = air.travel.data, aes(x = satisfaction, fill = Gender)) +
geom_bar() +
labs(title = "Level of Satisfaction by Class")
ggplot(data = air.travel.data, aes(x = satisfaction, fill = Gender)) +
geom_bar(position = "dodge") +
labs(title = "Level of Satisfaction by Class")
ggplot(data = air.travel.data, aes(x = satisfaction, fill = Customer.Type)) +
geom_bar(position = "dodge") +
labs(title = "Level of Satisfaction by Class")
### Neural Network
set.seed(99)
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/Final Project")
# Initiate Packages
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(ggplot2)
library(caret)
library(nnet)
library(NeuralNetTools)
library(ggformula)
library(reshape2)
ctrl = trainControl(method = "cv", number = 5)
ctrl = trainControl(method = "cv", number = 5)
fit_air.travel.data = train(satisfaction ~ ., data = air.travel.data,
method = "nnet",
tuneGrid = expand.grid(size = seq(1,10, by = 1),
decay = seq(0.1, 2, by = 0.1)),
skip = FALSE,
trace = FALSE,
preProc = c("center", "scale"),
maxit = 5000,
trControl = ctrl)
library(doParallel)
detectCores()
c1 <- makeCluster(10)
registerDoParallel(c1)
set.seed(99)
ctrl = trainControl(method = "cv", number = 5)
fit_air.travel.data = train(satisfaction ~ ., data = air.travel.data,
method = "nnet",
tuneGrid = expand.grid(size = seq(1,10, by = 1),
decay = seq(0.1, 2, by = 0.1)),
skip = FALSE,
trace = FALSE,
preProc = c("center", "scale"),
maxit = 5000,
trControl = ctrl)
stopCluster(c1)
dim(air.travel.data)
sqrt(23)
data_used = air.travel.data
ctrl = trainControl(method = "cv", number = 5)
air.travel.tree = train(satisfaction ~ .,
data = data_used,
method = "rf",
tuneGrid = expand.grid(mtry = c(5,6,7,8,9,10,15,20,23)),
trControl = ctrl)
stopCluster(c1)
c1 <- makeCluster(15)
registerDoParallel(c1)
data_used = air.travel.data
ctrl = trainControl(method = "cv", number = 5)
air.travel.tree = train(satisfaction ~ .,
data = data_used,
method = "rf",
tuneGrid = expand.grid(mtry = c(5,6,7,8,9,10,15,20,23)),
trControl = ctrl)
stopCluster(c1)
set.seed(99)
c1 <- makeCluster(15)
registerDoParallel(c1)
data_used = air.travel.data
ctrl = trainControl(method = "cv", number = 5)
air.travel.tree = train(satisfaction ~ .,
data = data_used,
method = "rf",
tuneGrid = expand.grid(mtry = c(5)),
trControl = ctrl)
stopCluster(c1)
library(randomForest)
plot(air.travel.tree)
air.travel.tree$bestTune
air.travel.tree$pred
air.travel.tree$finalModel
plot(air.travel.tree)
set.seed(99)
c1 <- makeCluster(15)
registerDoParallel(c1)
data_used = air.travel.data
ctrl = trainControl(method = "cv", number = 5)
air.travel.tree = train(satisfaction ~ .,
data = data_used,
method = "rf",
tuneGrid = expand.grid(mtry = c(5,6,7)),
trControl = ctrl)
stopCluster(c1)
plot(air.travel.tree)
set.seed(99)
c1 <- makeCluster(15)
registerDoParallel(c1)
data_used = air.travel.data
ctrl = trainControl(method = "cv", number = 5)
air.travel.tree = train(satisfaction ~ .,
data = data_used,
method = "rf",
tuneGrid = expand.grid(mtry = c(5,6,7,8,9,10)),
trControl = ctrl)
stopCluster(c1)
plot(air.travel.tree)
air.travel.tree$results
system.time({
set.seed(99)
c1 <- makeCluster(15)
registerDoParallel(c1)
data_used = air.travel.data
ctrl = trainControl(method = "cv", number = 5)
air.travel.tree = train(satisfaction ~ .,
data = data_used,
method = "rf",
tuneGrid = expand.grid(mtry = c(5,6,7,8,9,10,15,18,20,23)),
trControl = ctrl)
stopCluster(c1)
})
2918/60
plot(air.travel.tree)
### Random Forest
air.travel.bag = randomForest(satisfaction ~ ., data = air.travel.data,
mtry = 22, importance = TRUE)
### Random Forest
air.travel.bag = randomForest(satisfaction ~ ., data = air.travel.data,
mtry = 23, importance = TRUE)
### Random Forest
air.travel.bag = randomForest(satisfaction ~ ., data = air.travel.data,
mtry = 22, importance = TRUE)
str(air.travel.data)
# Fix types
air.travel.data <- air.travel.data %>%
mutate(Gender = as.factor(Gender))
str(air.travel.data)
# Fix types
air.travel.data <- air.travel.data %>%
mutate(Gender = as.factor(Gender),
Customer.Type = as.factor(Customer.Type),
Type.of.Travel = as.factor(Type.of.Travel),
Class = as.factor(Class),
satisfaction = as.factor(satisfaction))
str(air.travel.data)
air.travel.bag = randomForest(satisfaction ~ ., data = air.travel.data,
mtry = 22, importance = TRUE)
gc()
c1 <- makeCluster(15)
registerDoParallel(c1)
air.travel.bag = randomForest(satisfaction ~ ., data = air.travel.data,
mtry = 22, importance = TRUE)
stopCluster(c1)
importance(air.travel.bag)
varImpPlot(air.travel.bag)
View(air.travel.bag)
air.travel.bag
plot(air.travel.bag)
system.time({
air.travel.forest = randomForest(satisfaction ~ ., data = air.travel.data,
mtry = 22, importance = TRUE)
})
varImpPlot(air.travel.forest)
c1 <- makeCluster(15)
registerDoParallel(c1)
set.seed(99)
ctrl = trainControl(method = "cv", number = 5)
fit_air.travel.data = train(satisfaction ~ ., data = air.travel.data,
method = "nnet",
tuneGrid = expand.grid(size = seq(1,10, by = 1),
decay = 0.1),
skip = FALSE,
trace = FALSE,
preProc = c("center", "scale"),
maxit = 5000,
trControl = ctrl)
system.time({
c1 <- makeCluster(15)
registerDoParallel(c1)
set.seed(99)
ctrl = trainControl(method = "cv", number = 5)
fit_air.travel.data = train(satisfaction ~ ., data = air.travel.data,
method = "nnet",
tuneGrid = expand.grid(size = seq(1,10, by = 1),
decay = 0.1),
skip = FALSE,
trace = FALSE,
preProc = c("center", "scale"),
maxit = 5000,
trControl = ctrl)
stopCluster(c1)
})
1590/60
fit_air.travel.data$results
system.time({
c1 <- makeCluster(15)
registerDoParallel(c1)
set.seed(99)
ctrl = trainControl(method = "cv", number = 5)
fit_air.travel.data = train(satisfaction ~ ., data = air.travel.data,
method = "nnet",
tuneGrid = expand.grid(size = seq(1,5, by = 1),
decay = seq(0.1,1,by = 0.2)),
skip = FALSE,
trace = FALSE,
preProc = c("center", "scale"),
maxit = 5000,
trControl = ctrl)
stopCluster(c1)
})
warnings()
fit_air.travel.data$bestTune
